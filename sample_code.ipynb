{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "atlantic-willow",
   "metadata": {},
   "source": [
    "# Atari DRL Sample Code\n",
    "\n",
    "## Training Parameters\n",
    "\n",
    "If you want to see the Atari agent during training or just want to play around with the hyperparameters, here are the parameters that you can configure for training:\n",
    "\n",
    "* `gym_env`: Specifies the Atari environment. See this link: https://gym.openai.com/envs/#atari\n",
    "* `scaled_height`: Controls the scaling for the frame height during preprocessing.\n",
    "* `scaled_width`: Controls the scaling for the frame width during preprocessing.\n",
    "* `k_frames`: Controls how many frames are stack to represent one state.\n",
    "* `memory_size`: The maximum capacity of replay memory.\n",
    "* `memory_alpha`: Specifies how much priority to apply in replay memory.\n",
    "* `memory_beta`: Controls the importance sampling weights.\n",
    "* `memory_beta_increment`: The value at which beta is linearly annealed towards 1.\n",
    "* `memory_eps`: A value added to the priority to ensure an experience has a non-zero probability to be drawn.\n",
    "* `greedy_start`: The starting value for the exploration rate in the epsilon greedy policy.\n",
    "* `greedy_end`: The ending value for the exploration rate in the epsilon greedy policy.\n",
    "* `greedy_decay`: The value at which the exploration rate is linearly annealed towards `greedy_end`.\n",
    "* `num_episodes`: The total number of episodes that the agent will train for.\n",
    "* `max_timesteps`: The maximum number of states that the agent can experience for each episode.\n",
    "* `discount`: The discount factor in the Q-learning algorithm.\n",
    "* `batch_size`: The batch size used for training.\n",
    "* `target_update`: The number of episodes that must pass for a target update to occur on the target network.\n",
    "* `optim_lr`: The learning rate used in the Adam optimizer.\n",
    "* `optim_eps`: The epsilon value used in the Adam optimizer.\n",
    "* `render`: Determines if the Atari environment is rendered during training or not.\n",
    "* `plot_reward`: Determines if the reward for each episode is plotted during training.\n",
    "* `save_rewards`: Determines if the mean rewards for each episode is saved on disk.\n",
    "* `save_model`: Determines if the target network's state dictionary will be saved to disk after training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "casual-interview",
   "metadata": {},
   "source": [
    "### Import Statements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "apparent-cloud",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from atari import DQN, AtariAI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dedicated-basic",
   "metadata": {},
   "source": [
    "### Training with DQN without Prioritized Experience Replay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "headed-classroom",
   "metadata": {},
   "outputs": [],
   "source": [
    "gym_env = 'Pong-v0'\n",
    "scaled_height = 84\n",
    "scaled_width = 84\n",
    "k_frames = 4\n",
    "memory_size = 10000\n",
    "greedy_start = 1.\n",
    "greedy_end = 0.01\n",
    "greedy_decay = 1.5e-3\n",
    "num_episodes = 100\n",
    "max_timesteps = 10000\n",
    "discount = 0.99\n",
    "batch_size = 32\n",
    "target_update = 10\n",
    "optim_lr = 2.5e-4\n",
    "optim_eps = 1e-8\n",
    "render = True\n",
    "plot_reward = True\n",
    "save_rewards = True\n",
    "save_model = True\n",
    "\n",
    "AtariAI.train_DQN(gym_env, scaled_height, scaled_width, k_frames, memory_size, greedy_start, greedy_end,\n",
    "              greedy_decay, num_episodes, max_timesteps, discount, batch_size, target_update, optim_lr, \n",
    "              optim_eps, render, plot_reward, save_rewards, save_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "athletic-titanium",
   "metadata": {},
   "source": [
    "### Training with DDQN and Prioritized Experience Replay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vocational-combat",
   "metadata": {},
   "outputs": [],
   "source": [
    "gym_env = 'Pong-v0'\n",
    "scaled_height = 84\n",
    "scaled_width = 84\n",
    "k_frames = 4\n",
    "memory_size = 50000\n",
    "memory_alpha = 0.4\n",
    "memory_beta = 0.4\n",
    "memory_beta_increment = 1.5e-5\n",
    "memory_eps = 1e-2\n",
    "greedy_start = 1.\n",
    "greedy_end = 0.01\n",
    "greedy_decay = 1.5e-3\n",
    "num_episodes = 100\n",
    "max_timesteps = 10000\n",
    "discount = 0.99\n",
    "batch_size = 32\n",
    "target_update = 10\n",
    "optim_lr = 2.5e-4\n",
    "optim_eps = 1e-8\n",
    "render = True\n",
    "plot_reward = True\n",
    "save_rewards = True\n",
    "save_model = True\n",
    "\n",
    "AtariAI.train_DDQN_PER(gym_env, scaled_height, scaled_width, k_frames, memory_size, memory_alpha,\n",
    "              memory_beta, memory_beta_increment, memory_eps, greedy_start, greedy_end,\n",
    "              greedy_decay, num_episodes, max_timesteps, discount, batch_size, target_update,\n",
    "              optim_lr, optim_eps, render, plot_reward, save_rewards, save_model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "atari",
   "language": "python",
   "name": "atari"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
